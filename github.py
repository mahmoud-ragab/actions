import requests
import threading
import time
import re
from bs4 import BeautifulSoup
from queue import Queue
import itertools
import string
import zipfile
import io
import csv

# ğŸ”‘ æ–°å¢ wordfreq ç”¨äºåŠ è½½ 100 ä¸‡é«˜é¢‘è¯
from wordfreq import top_n_list  

lock = threading.Lock()

seen_keywords = set()
seen_schools = set()
results = set()
domains_only = set()

# åŸºç¡€å…³é”®è¯
base_keywords = [
    "university", "college", "institute", "faculty", "polytechnic", "campus", "school",
    "universidad", "universite", "hochschule", "akademia", "teknik", "technological",
    "indonesia", "philippines", "thailand", "beijing", "hong kong", "tokyo", "malaysia",
    "engineering", "medical", "technology", "science", "national", "china", "japan",
    "korea", "taiwan", "singapore", "brazil", "india", "germany", "france", "canada",
    "primary", "secondary", "elementary", "highschool", "kindergarten", "middle school",
    "faculty", "education", "academy", "universitÃ¤t", "Ã©cole", "escuela", "ÑˆĞºĞ¾Ğ»Ğ°", "å­¦æ ¡", "ëŒ€í•™êµ",
    "universitÃ ", "universidade", "skola", "skole", "lyceum", "college of", "institute of",
    # æ•™è‚²ç›¸å…³èŒä½
    "principal", "headmaster", "dean", "professor", "lecturer", "tutor", "counselor",
    "registrar", "chancellor", "provost", "superintendent", "trustee", "faculty member",
    "staff", "coach",
    # å­¦ç§‘ä¸“ä¸š
    "biotechnology", "data science", "artificial intelligence", "cybersecurity",
    "renewable energy", "urban planning", "marine biology", "forensic science",
    "speech therapy", "social work", "graphic design", "culinary arts", "veterinary science",
    "library science",
    # å»ºç­‘è®¾æ–½
    "library", "laboratory", "auditorium", "gymnasium", "dormitory", "cafeteria",
    "research center", "sports complex", "student center", "innovation hub", "media center",
    # æ•™è‚²é˜¶æ®µç±»å‹
    "preschool", "kindergarten", "elementary school", "middle school", "junior high",
    "senior high", "vocational school", "adult education", "special education",
    "online courses", "continuing education", "night school",
    # å­¦æœ¯æ´»åŠ¨
    "seminar", "workshop", "conference", "exchange program", "study abroad",
    "internship program", "scholarship program", "summer school", "online learning",
    "distance education", "research project", "alumni association",
    # è¡Œæ”¿åŒºåˆ’
    "village", "hamlet", "neighborhood", "ward", "block", "precinct", "suburb",
    "township", "canton", "parish",
    # è¯­è¨€æ–‡åŒ–
    "bilingual", "trilingual", "language center", "cultural center",
    "international school", "immersion program", "heritage school",
    # å…¶ä»–ç›¸å…³
    "education reform", "curriculum development", "standards",
    "charter school", "magnet school", "alternative school",
    "accreditation", "qs ranking", "times higher education",
    "online platform", "learning management system", "virtual classroom"
]

# ç”Ÿæˆæ‰€æœ‰3å­—æ¯ç»„åˆå…³é”®è¯
def generate_letter_combinations(min_len=3, max_len=3):
    chars = string.ascii_lowercase
    for length in range(min_len, max_len+1):
        for combo in itertools.product(chars, repeat=length):
            yield ''.join(combo)

# GeoNamesåœ¨çº¿åŠ è½½å›½å®¶ã€çœ/å·å…³é”®è¯
def load_countries():
    url = "https://download.geonames.org/export/dump/countryInfo.txt"
    countries = []
    try:
        resp = requests.get(url, timeout=15)
        resp.raise_for_status()
        for line in resp.text.splitlines():
            if line.startswith('#') or not line.strip():
                continue
            parts = line.split('\t')
            if len(parts) >= 5:
                countries.append(parts[4].lower())
    except Exception as e:
        print(f"[!] åŠ è½½å›½å®¶å¤±è´¥: {e}")
    return countries

def load_admin1():
    url = "https://download.geonames.org/export/dump/admin1CodesASCII.txt"
    admins = []
    try:
        resp = requests.get(url, timeout=15)
        resp.raise_for_status()
        for line in resp.text.splitlines():
            parts = line.split('\t')
            if len(parts) >= 2:
                admins.append(parts[1].lower())
    except Exception as e:
        print(f"[!] åŠ è½½çœ/å·å¤±è´¥: {e}")
    return admins

def load_cities():
    url = "https://download.geonames.org/export/dump/cities1000.zip"
    cities = []
    try:
        resp = requests.get(url, timeout=30)
        resp.raise_for_status()
        z = zipfile.ZipFile(io.BytesIO(resp.content))
        with z.open("cities1000.txt") as f:
            for line in io.TextIOWrapper(f, encoding='utf-8'):
                parts = line.split('\t')
                if len(parts) >= 2:
                    city = parts[1].strip().lower()
                    if city:
                        cities.append(city)
    except Exception as e:
        print(f"[!] åŠ è½½åŸå¸‚å¤±è´¥: {e}")
    return cities

def load_geo_keywords():
    print("[*] åŠ è½½å›½å®¶å…³é”®è¯...")
    countries = load_countries()
    print(f"[*] å›½å®¶æ•°: {len(countries)}")
    print("[*] åŠ è½½çœ/å·å…³é”®è¯...")
    admins = load_admin1()
    print(f"[*] çœ/å·æ•°: {len(admins)}")
    print("[*] åŠ è½½åŸå¸‚å…³é”®è¯ï¼ˆå¤§æ–‡ä»¶ï¼Œéœ€è€å¿ƒï¼‰...")
    cities = load_cities()
    print(f"[*] åŸå¸‚æ•°: {len(cities)}")

    all_geo = set()
    all_geo.update(countries)
    all_geo.update(admins)
    all_geo.update(cities)
    return list(all_geo)

# ==========================================================
# è¿™é‡Œæ›¿æ¢å…³é”®è¯åˆå§‹åŒ–é€»è¾‘ï¼šåŠ è½½ GitHub 46 ä¸‡ + wordfreq 100 ä¸‡
# ==========================================================
def load_big_wordlist():
    print("[*] æ­£åœ¨åŠ è½½ GitHub è¯è¡¨...")
    url = "https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
    resp = requests.get(url, timeout=30)
    resp.raise_for_status()
    words_github = set(resp.text.splitlines())

    print("[*] æ­£åœ¨åŠ è½½ wordfreq é«˜é¢‘è¯...")
    words_wordfreq = set(top_n_list("en", 1000000))

    merged = words_github | words_wordfreq
    print(f"[*] GitHub + wordfreq åˆå¹¶åå…³é”®è¯æ•°: {len(merged)}")
    return merged

print("[*] åˆå§‹åŒ–å…³é”®è¯...")
initial_keywords = set(base_keywords)
initial_keywords.update(generate_letter_combinations(3,3))
initial_keywords.update(load_geo_keywords())
initial_keywords.update(load_big_wordlist())  # ğŸ”¥ åŠ å…¥ 150 ä¸‡è¯
initial_keywords = list(initial_keywords)
print(f"[*] æœ€ç»ˆåˆå§‹å…³é”®è¯æ€»æ•°: {len(initial_keywords)}")

# ==========================================================
# ä»¥ä¸‹éƒ¨åˆ†ä¿æŒåŸæ ·ï¼ˆGitHub æœç´¢é€»è¾‘ï¼‰
# ==========================================================

HEADERS = {
    "accept": "text/fragment+html",
    "accept-language": "en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7",
    "referer": "https://github.com/settings/education/benefits",
    "sec-ch-ua": "\"Google Chrome\";v=\"137\", \"Chromium\";v=\"137\", \"Not/A)Brand\";v=\"24\"",
    "sec-ch-ua-mobile": "?0",
    "sec-ch-ua-platform": "\"Windows\"",
    "sec-fetch-dest": "empty",
    "sec-fetch-mode": "cors",
    "sec-fetch-site": "same-origin",
    "cookie": "__Host-user_session_same_site=Dl9dHRcLP09rGw6VxYXJnvAY2G0tn6f5oCWdlVdcKAF-M87k;_device_id=f2e8347acf71d1d5984b1bb5503df6e8;_gh_sess=xfjiHQrUEDLljKsd%2BeW2Fz7UOiz2qGszjpE9%2FCP68HFCYsbkAH8esvQug4XmA2SNJvseuQbgdE3%2B89gC80OwzixCF9J%2FqjE5y3chSqvf30ACriCd%2BxLT1cqyMOEBDpe1Lv2IJ%2BH2FVe1c2Kch4%2Fo6iBzt26mYRJXm8nsFkAsHmmz8glUPhxl4gfItJa5%2BmHrKYRTIUwm%2BO2T4FUY%2BtkV7ov83F55fLefRBVU1n91yT8KnYN2UZ5SyOAQtK1FitX3NpSOJgkX0dduU4zknoE5GaRcyoIr5TJQwyE7SRVmULvJjOKwFB8PPoTny887H%2FK0Z%2BeUn9Z2s9YB1QgUQjlg7CZZ414MbMO1cNfJew%3D%3D--1f8440q3WIvz5wcp--pUquBUHEktxIidsI9BcUCQ%3D%3D;_octo=GH1.1.1220093520.1750220192;color_mode=%7B%22color_mode%22%3A%22auto%22%2C%22light_theme%22%3A%7B%22name%22%3A%22light%22%2C%22color_mode%22%3A%22light%22%7D%2C%22dark_theme%22%3A%7B%22name%22%3A%22dark%22%2C%22color_mode%22%3A%22dark%22%7D%7D;cpu_bucket=xlg;dotcom_user=mahmoud-ragab;GHCC=Required:1-Analytics:1-SocialMedia:1-Advertising:1;logged_in=yes;MicrosoftApplicationsTelemetryDeviceId=ea18b101-56c1-4dd0-98bd-1385451cad05;preferred_color_mode=light;saved_user_sessions=82536688%3AUIkX4SyFfoloKdbOrUSizOOZq7snwfIm7HoYQ1cAz3qtU3Sq%7C18014637%3ADl9dHRcLP09rGw6VxYXJnvAY2G0tn6f5oCWdlVdcKAF-M87k;tz=Asia%2FShanghai;user_session=Dl9dHRcLP09rGw6VxYXJnvAY2G0tn6f5oCWdlVdcKAF-M87k"
}

BASE_URL = "https://github.com/settings/education/developer_pack_applications/schools?q="

q = Queue()
num_threads = 30
threads = []

def worker():
    while True:
        kw = q.get()
        if kw is None:
            break
        search_keyword(kw)
        q.task_done()

def search_keyword(keyword):
    if keyword in seen_keywords:
        return
    seen_keywords.add(keyword)

    first_429 = True
    while True:
        try:
            print(f"[*] æœç´¢å…³é”®è¯: {keyword}")
            url = BASE_URL + requests.utils.quote(keyword)
            resp = requests.get(url, headers=HEADERS, timeout=15)

            if resp.status_code == 429:
                if first_429:
                    print(f"[!] è¢«é™æµ 429ï¼Œå…³é”®è¯'{keyword}'ä¼‘çœ 70åˆ†é’Ÿ...")
                    time.sleep(4200)
                    first_429 = False
                else:
                    print(f"[!] å†æ¬¡é™æµ 429ï¼Œå…³é”®è¯'{keyword}'ä¼‘çœ 5åˆ†é’Ÿ...")
                    time.sleep(300)
                continue

            if resp.status_code != 200:
                print(f"[!] è¯·æ±‚å¤±è´¥ {resp.status_code}ï¼Œå…³é”®è¯: {keyword}")
                return

            soup = BeautifulSoup(resp.text, "html.parser")
            items = soup.find_all("div", class_="ActionListItem typeahead-result js-school-autocomplete-result-selection")

            for item in items:
                school = item.get("data-school-name")
                domains_raw = item.get("data-email-domains")

                if not school or school in seen_schools:
                    continue

                seen_schools.add(school)

                if domains_raw and domains_raw != "[]":
                    domains_str = domains_raw.replace("&quot;", '"').replace("false", "False").replace("true", "True")
                    try:
                        domains_list = eval(domains_str)
                    except Exception as e:
                        print(f"[!] åŸŸåè§£æå¤±è´¥: {e}")
                        domains_list = []

                    if domains_list:
                        for domain_info in domains_list:
                            domain = domain_info[0]
                            entry = f"{domain}--{school}"
                            with lock:
                                if entry not in results:
                                    results.add(entry)
                                    domains_only.add(domain)
                                    print("[+] å‘ç°å­¦æ ¡:", entry)
                                    results_file.write(entry + "\n")
                                    domains_file.write(domain + "\n")
                                    results_file.flush()
                                    domains_file.flush()

                    # æ‹†åˆ†å­¦æ ¡åé€’å½’åŠ å…¥å…³é”®è¯é˜Ÿåˆ—
                    words = re.split(r"[,\s\-â€“]", school)
                    for w in words:
                        w = w.strip().lower()
                        if w and w not in seen_keywords and 2 < len(w) < 40:
                            q.put(w)

                else:
                    words = re.split(r"[,\s\-â€“]", school)
                    for w in words:
                        w = w.strip().lower()
                        if w and w not in seen_keywords and 2 < len(w) < 40:
                            q.put(w)
            break

        except requests.exceptions.RequestException as e:
            if first_429:
                print(f"[!] è¯·æ±‚å¼‚å¸¸ {keyword}: {e}ï¼Œä¼‘çœ 70åˆ†é’Ÿé‡è¯•")
                time.sleep(4200)
                first_429 = False
            else:
                print(f"[!] è¯·æ±‚å¼‚å¸¸ {keyword}: {e}ï¼Œä¼‘çœ 5åˆ†é’Ÿé‡è¯•")
                time.sleep(300)

if __name__ == "__main__":
    open("results_full.txt", "w", encoding="utf-8").close()
    open("results_domains.txt", "w", encoding="utf-8").close()

    results_file = open("results_full.txt", "a", encoding="utf-8")
    domains_file = open("results_domains.txt", "a", encoding="utf-8")

    for kw in initial_keywords:
        q.put(kw)

    for _ in range(num_threads):
        t = threading.Thread(target=worker)
        t.daemon = True
        t.start()
        threads.append(t)

    try:
        q.join()
    except KeyboardInterrupt:
        print("[*] æ‰‹åŠ¨ç»ˆæ­¢ï¼")

    for _ in threads:
        q.put(None)
    for t in threads:
        t.join()

    results_file.close()
    domains_file.close()

    print("[âœ“] å®Œæˆã€‚ç»“æœä¿å­˜åˆ° results_full.txt å’Œ results_domains.txt")
